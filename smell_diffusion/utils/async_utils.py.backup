"""Advanced asynchronous utilities for high-performance concurrent processing."""

import asyncio
import concurrent.futures
import threading
import time
import weakref
import os
from typing import List, Callable, Any, Optional, Dict, Awaitable, Union, Tuple
from functools import wraps
from dataclasses import dataclass
from collections import defaultdict, deque
import heapq
import uuid
import hashlib

from .logging import SmellDiffusionLogger, performance_monitor
from .caching import get_cache
from .config import get_config


@dataclass
class TaskPriority:
    """Task priority levels for queue management."""
    CRITICAL = 0
    HIGH = 1
    NORMAL = 2
    LOW = 3
    BACKGROUND = 4


@dataclass
class AsyncTask:
    """Represents an asynchronous task with priority and metadata."""
    task_id: str
    priority: int
    func: Callable
    args: tuple
    kwargs: dict
    created_at: float
    timeout: Optional[float] = None
    
    def __lt__(self, other):
        # For priority queue ordering
        if self.priority != other.priority:
            return self.priority < other.priority
        return self.created_at < other.created_at


class AdvancedAsyncExecutor:
    """High-performance async executor with intelligent task scheduling."""
    
    def __init__(self, max_workers: int = None, enable_adaptive_scaling: bool = True):
        """Initialize advanced executor."""
        self.config = get_config()
        self.max_workers = max_workers or min(32, (os.cpu_count() or 1) + 4)
        self.enable_adaptive_scaling = enable_adaptive_scaling
        self.logger = SmellDiffusionLogger("advanced_executor")
        
        # Task scheduling
        self.task_queue = asyncio.PriorityQueue()
        self.active_tasks: Dict[str, asyncio.Task] = {}
        self.completed_tasks: Dict[str, Any] = {}
        self.failed_tasks: Dict[str, Exception] = {}
        
        # Performance tracking
        self.task_metrics = defaultdict(lambda: {
            'count': 0,
            'total_time': 0.0,
            'avg_time': 0.0,
            'errors': 0
        })
        
        # Adaptive scaling
        self.worker_pool = []
        self.current_workers = 0
        self.target_workers = max(2, self.max_workers // 4)
        self.last_scaling_time = 0
        self.scaling_cooldown = 30  # seconds
        
        # Circuit breaker for resilience
        self.circuit_breakers: Dict[str, 'CircuitBreaker'] = {}
        
        # Start background worker management
        self._worker_manager_task = None
        self._metrics_collector_task = None
        
    async def start(self):
        """Start the executor and background tasks."""
        self.logger.logger.info(f"Starting advanced executor with {self.max_workers} max workers")
        
        # Start initial workers
        await self._scale_workers(self.target_workers)
        
        # Start background management tasks
        self._worker_manager_task = asyncio.create_task(self._worker_manager())
        self._metrics_collector_task = asyncio.create_task(self._metrics_collector())
        
        self.logger.logger.info("Advanced executor started successfully")
    
    async def stop(self):
        """Stop the executor gracefully."""
        self.logger.logger.info("Stopping advanced executor...")
        
        # Cancel background tasks
        if self._worker_manager_task:
            self._worker_manager_task.cancel()
        if self._metrics_collector_task:
            self._metrics_collector_task.cancel()
        
        # Cancel all active tasks
        for task in self.active_tasks.values():
            task.cancel()
        
        # Wait for workers to finish
        if self.worker_pool:
            await asyncio.gather(*self.worker_pool, return_exceptions=True)
        
        self.logger.logger.info("Advanced executor stopped")\n    \n    async def submit_task(self, \n                         func: Callable, \n                         *args, \n                         priority: int = TaskPriority.NORMAL,\n                         timeout: Optional[float] = None,\n                         **kwargs) -> str:\n        """Submit a task for execution."""
        task_id = str(uuid.uuid4())\n        task = AsyncTask(\n            task_id=task_id,\n            priority=priority,\n            func=func,\n            args=args,\n            kwargs=kwargs,\n            created_at=time.time(),\n            timeout=timeout\n        )\n        \n        await self.task_queue.put(task)\n        self.logger.logger.debug(f"Submitted task {task_id} with priority {priority}")\n        \n        return task_id\n    \n    async def get_result(self, task_id: str, timeout: Optional[float] = None) -> Any:\n        """Get task result by ID."""
        start_time = time.time()\n        \n        while True:\n            # Check if completed\n            if task_id in self.completed_tasks:\n                return self.completed_tasks.pop(task_id)\n            \n            # Check if failed\n            if task_id in self.failed_tasks:\n                error = self.failed_tasks.pop(task_id)\n                raise error\n            \n            # Check timeout\n            if timeout and (time.time() - start_time) > timeout:\n                raise asyncio.TimeoutError(f"Task {task_id} timed out")\n            \n            # Wait a bit before checking again\n            await asyncio.sleep(0.1)\n    \n    async def execute_and_wait(self, \n                              func: Callable, \n                              *args, \n                              priority: int = TaskPriority.NORMAL,\n                              timeout: Optional[float] = None,\n                              **kwargs) -> Any:\n        """Submit task and wait for result."""
        task_id = await self.submit_task(func, *args, priority=priority, timeout=timeout, **kwargs)\n        return await self.get_result(task_id, timeout)\n    \n    async def _worker_manager(self):\n        """Manage worker pool dynamically."""
        while True:\n            try:\n                await asyncio.sleep(10)  # Check every 10 seconds\n                \n                if self.enable_adaptive_scaling:\n                    await self._adaptive_scaling()\n                \n                # Clean up completed tasks periodically\n                await self._cleanup_old_tasks()\n                \n            except asyncio.CancelledError:\n                break\n            except Exception as e:\n                self.logger.log_error("worker_manager", e)\n    \n    async def _adaptive_scaling(self):\n        """Implement adaptive worker scaling based on load."""
        current_time = time.time()\n        \n        # Avoid frequent scaling changes\n        if current_time - self.last_scaling_time < self.scaling_cooldown:\n            return\n        \n        queue_size = self.task_queue.qsize()\n        active_count = len(self.active_tasks)\n        \n        # Scale up if queue is growing and we have capacity\n        if queue_size > self.current_workers * 2 and self.current_workers < self.max_workers:\n            new_target = min(self.max_workers, self.current_workers + 2)\n            await self._scale_workers(new_target)\n            self.last_scaling_time = current_time\n            self.logger.logger.info(f"Scaled up to {new_target} workers (queue: {queue_size})")\n        \n        # Scale down if queue is empty and we have excess workers\n        elif queue_size == 0 and active_count < self.current_workers // 2 and self.current_workers > 2:\n            new_target = max(2, self.current_workers - 1)\n            await self._scale_workers(new_target)\n            self.last_scaling_time = current_time\n            self.logger.logger.info(f"Scaled down to {new_target} workers")\n    \n    async def _scale_workers(self, target_count: int):\n        """Scale worker pool to target count."""
        if target_count > self.current_workers:\n            # Add workers\n            for _ in range(target_count - self.current_workers):\n                worker = asyncio.create_task(self._worker())\n                self.worker_pool.append(worker)\n        \n        elif target_count < self.current_workers:\n            # Remove workers (they will finish current tasks and exit)\n            excess = self.current_workers - target_count\n            for _ in range(excess):\n                if self.worker_pool:\n                    worker = self.worker_pool.pop()\n                    worker.cancel()\n        \n        self.current_workers = target_count\n        self.target_workers = target_count\n    \n    async def _worker(self):\n        """Worker coroutine that processes tasks from the queue."""
        worker_id = str(uuid.uuid4())[:8]\n        self.logger.logger.debug(f"Worker {worker_id} started")\n        \n        try:\n            while True:\n                try:\n                    # Get next task with timeout\n                    task = await asyncio.wait_for(self.task_queue.get(), timeout=30.0)\n                    \n                    # Execute task\n                    await self._execute_task(task)\n                    \n                except asyncio.TimeoutError:\n                    # Worker idle timeout - exit gracefully\n                    break\n                except asyncio.CancelledError:\n                    break\n                except Exception as e:\n                    self.logger.log_error(f"worker_{worker_id}", e)\n        \n        finally:\n            self.logger.logger.debug(f"Worker {worker_id} stopped")\n    \n    async def _execute_task(self, task: AsyncTask):\n        """Execute an individual task."""
        start_time = time.time()\n        func_name = getattr(task.func, '__name__', str(task.func))\n        \n        try:\n            # Add to active tasks\n            self.active_tasks[task.task_id] = task\n            \n            # Check circuit breaker\n            circuit_breaker = self._get_circuit_breaker(func_name)\n            if not circuit_breaker.can_execute():\n                raise RuntimeError(f"Circuit breaker open for {func_name}")\n            \n            # Execute with timeout\n            if asyncio.iscoroutinefunction(task.func):\n                result = await asyncio.wait_for(\n                    task.func(*task.args, **task.kwargs),\n                    timeout=task.timeout\n                )\n            else:\n                # Run in thread pool for sync functions\n                loop = asyncio.get_event_loop()\n                result = await asyncio.wait_for(\n                    loop.run_in_executor(\n                        None, \n                        lambda: task.func(*task.args, **task.kwargs)\n                    ),\n                    timeout=task.timeout\n                )\n            \n            # Mark as successful\n            circuit_breaker.record_success()\n            self.completed_tasks[task.task_id] = result\n            \n            # Update metrics\n            execution_time = time.time() - start_time\n            self._update_metrics(func_name, execution_time, success=True)\n            \n            self.logger.logger.debug(f"Task {task.task_id} completed in {execution_time:.3f}s")\n            \n        except Exception as e:\n            # Mark as failed\n            circuit_breaker.record_failure()\n            self.failed_tasks[task.task_id] = e\n            \n            # Update metrics\n            execution_time = time.time() - start_time\n            self._update_metrics(func_name, execution_time, success=False)\n            \n            self.logger.log_error(f"task_{task.task_id}", e, {\n                "func_name": func_name,\n                "execution_time": execution_time\n            })\n        \n        finally:\n            # Remove from active tasks\n            self.active_tasks.pop(task.task_id, None)\n    \n    def _get_circuit_breaker(self, func_name: str) -> 'CircuitBreaker':\n        """Get or create circuit breaker for function."""
        if func_name not in self.circuit_breakers:\n            self.circuit_breakers[func_name] = CircuitBreaker(\n                failure_threshold=5,\n                recovery_timeout=60,\n                expected_exception=Exception\n            )\n        return self.circuit_breakers[func_name]


class CircuitBreaker:
    """Circuit breaker for fault tolerance."""
    
    def __init__(self, failure_threshold: int = 5, recovery_timeout: int = 60, expected_exception: type = Exception):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.expected_exception = expected_exception
        
        self.failure_count = 0
        self.last_failure_time = None
        self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN
        
        self.logger = SmellDiffusionLogger("circuit_breaker")
    
    def can_execute(self) -> bool:
        """Check if execution is allowed."""
        if self.state == 'CLOSED':
            return True
        
        if self.state == 'OPEN':
            if self.last_failure_time and time.time() - self.last_failure_time > self.recovery_timeout:
                self.state = 'HALF_OPEN'
                self.logger.logger.info("Circuit breaker entering HALF_OPEN state")
                return True
            return False
        
        # HALF_OPEN state
        return True
    
    def record_success(self):
        """Record successful execution."""
        if self.state == 'HALF_OPEN':
            self.state = 'CLOSED'
            self.failure_count = 0
            self.logger.logger.info("Circuit breaker reset to CLOSED state")
    
    def record_failure(self):
        """Record failed execution."""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            self.state = 'OPEN'
            self.logger.logger.warning(f"Circuit breaker opened after {self.failure_count} failures")
    
    def get_status(self) -> Dict[str, Any]:
        """Get circuit breaker status."""
        return {
            'state': self.state,
            'failure_count': self.failure_count,
            'failure_threshold': self.failure_threshold,
            'last_failure_time': self.last_failure_time
        }\n    \n    def _update_metrics(self, func_name: str, execution_time: float, success: bool):\n        """Update performance metrics."""
        metrics = self.task_metrics[func_name]\n        metrics['count'] += 1\n        \n        if success:\n            metrics['total_time'] += execution_time\n            metrics['avg_time'] = metrics['total_time'] / (metrics['count'] - metrics['errors'])\n        else:\n            metrics['errors'] += 1\n    \n    async def _cleanup_old_tasks(self):\n        """Clean up old completed/failed tasks to prevent memory leaks."""
        current_time = time.time()\n        cleanup_age = 3600  # 1 hour\n        \n        # Clean completed tasks\n        old_completed = [\n            task_id for task_id in self.completed_tasks.keys()\n            if current_time - self.completed_tasks.get(task_id, {}).get('timestamp', current_time) > cleanup_age\n        ]\n        for task_id in old_completed:\n            self.completed_tasks.pop(task_id, None)\n        \n        # Clean failed tasks\n        old_failed = [\n            task_id for task_id in self.failed_tasks.keys()\n            if current_time - getattr(self.failed_tasks.get(task_id), 'timestamp', current_time) > cleanup_age\n        ]\n        for task_id in old_failed:\n            self.failed_tasks.pop(task_id, None)\n    \n    async def _metrics_collector(self):\n        """Collect and log performance metrics."""
        while True:\n            try:\n                await asyncio.sleep(300)  # Every 5 minutes\n                \n                # Log current status\n                queue_size = self.task_queue.qsize()\n                active_count = len(self.active_tasks)\n                \n                self.logger.logger.info(\n                    f"Executor status: {self.current_workers} workers, "\n                    f"{queue_size} queued, {active_count} active"\n                )\n                \n                # Log function metrics\n                for func_name, metrics in self.task_metrics.items():\n                    if metrics['count'] > 0:\n                        error_rate = metrics['errors'] / metrics['count']\n                        self.logger.logger.info(\n                            f"Function {func_name}: {metrics['count']} calls, "\n                            f"avg {metrics['avg_time']:.3f}s, {error_rate:.1%} errors"\n                        )\n                \n            except asyncio.CancelledError:\n                break\n            except Exception as e:\n                self.logger.log_error("metrics_collector", e)\n    \n    def get_status(self) -> Dict[str, Any]:\n        """Get current executor status."""
        return {\n            "workers": {\n                "current": self.current_workers,\n                "target": self.target_workers,\n                "max": self.max_workers\n            },\n            "tasks": {\n                "queued": self.task_queue.qsize(),\n                "active": len(self.active_tasks),\n                "completed": len(self.completed_tasks),\n                "failed": len(self.failed_tasks)\n            },\n            "metrics": dict(self.task_metrics),\n            "circuit_breakers": {\n                name: cb.get_status() \n                for name, cb in self.circuit_breakers.items()\n            }\n        }


class AsyncMoleculeGenerator:
    """High-performance asynchronous molecule generator with advanced features."""
    
    def __init__(self, base_generator, max_workers: int = None, enable_caching: bool = True):
        """Initialize async generator with advanced features."""
        self.base_generator = base_generator
        self.max_workers = max_workers or min(16, (os.cpu_count() or 1) + 4)
        self.enable_caching = enable_caching
        self.logger = SmellDiffusionLogger("async_molecule_generator")
        
        # Advanced executor for high-performance processing
        self.executor = AdvancedAsyncExecutor(max_workers=self.max_workers)
        
        # Caching for performance
        self.cache = get_cache() if enable_caching else None
        
        # Request deduplication
        self.pending_requests: Dict[str, List[asyncio.Future]] = defaultdict(list)
        
        # Performance optimization
        self.batch_processor = None
        
    async def start(self):\n        """Start the async generator."""
        await self.executor.start()\n        self.logger.logger.info("Async molecule generator started")\n    \n    async def stop(self):\n        """Stop the async generator."""
        await self.executor.stop()\n        self.logger.logger.info("Async molecule generator stopped")
    
    async def generate_async(self, prompt: str, num_molecules: int = 1, **kwargs) -> List[Any]:
        """Generate molecules asynchronously."""
        loop = asyncio.get_event_loop()
        
        try:
            result = await loop.run_in_executor(
                self.executor,
                self._generate_sync,
                prompt,
                num_molecules,
                kwargs
            )
            return result
        except Exception as e:
            self.logger.log_error("async_generation", e, {"prompt": prompt})
            return []
    
    def _generate_sync(self, prompt: str, num_molecules: int, kwargs: Dict[str, Any]) -> List[Any]:
        """Synchronous generation wrapper."""
        result = self.base_generator.generate(
            prompt=prompt,
            num_molecules=num_molecules,
            **kwargs
        )
        return result if isinstance(result, list) else [result] if result else []
    
    async def batch_generate_async(self, prompts: List[str], **kwargs) -> List[List[Any]]:
        """Generate molecules for multiple prompts concurrently."""
        tasks = []
        
        for prompt in prompts:
            task = self.generate_async(prompt, **kwargs)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Handle exceptions in results
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                self.logger.log_error("batch_item_error", result, {"prompt_index": i})
                processed_results.append([])
            else:
                processed_results.append(result)
        
        return processed_results
    
    def __del__(self):
        """Cleanup executor on deletion."""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=True)


class ConcurrentSafetyEvaluator:
    """Concurrent safety evaluation for multiple molecules."""
    
    def __init__(self, base_evaluator, max_workers: int = 8):
        """Initialize concurrent evaluator."""
        self.base_evaluator = base_evaluator
        self.max_workers = max_workers
        self.logger = SmellDiffusionLogger("concurrent_safety")
    
    @performance_monitor("concurrent_safety_evaluation")
    def evaluate_batch(self, molecules: List[Any]) -> List[Dict[str, Any]]:
        """Evaluate safety of multiple molecules concurrently."""
        if not molecules:
            return []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all evaluation tasks
            future_to_molecule = {
                executor.submit(self._safe_evaluate, mol): mol
                for mol in molecules if mol is not None
            }
            
            results = []
            for future in concurrent.futures.as_completed(future_to_molecule):
                mol = future_to_molecule[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    self.logger.log_error("safety_evaluation", e, {"molecule": str(mol)})
                    results.append(None)
            
            return results
    
    def _safe_evaluate(self, molecule) -> Optional[Dict[str, Any]]:
        """Safely evaluate a single molecule."""
        try:
            if hasattr(molecule, 'get_safety_profile'):
                safety = molecule.get_safety_profile()
                return {
                    "molecule_smiles": molecule.smiles,
                    "score": safety.score,
                    "ifra_compliant": safety.ifra_compliant,
                    "allergens": safety.allergens,
                    "warnings": safety.warnings
                }
            return None
        except Exception as e:
            self.logger.log_error("single_safety_evaluation", e)
            return None


class AsyncCacheManager:
    """Asynchronous cache operations."""
    
    def __init__(self):
        """Initialize async cache manager."""
        self.cache = get_cache()
        self.logger = SmellDiffusionLogger("async_cache")
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)
    
    async def get_async(self, key: str) -> Optional[Any]:
        """Get value from cache asynchronously."""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.executor, self.cache.get, key)
    
    async def set_async(self, key: str, value: Any, ttl: int = 3600, persist: bool = True) -> None:
        """Set value in cache asynchronously."""
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            self.executor,
            lambda: self.cache.set(key, value, ttl, persist)
        )
    
    async def preload_cache(self, keys_and_generators: List[tuple]) -> None:
        """Preload cache with multiple key-value pairs."""
        tasks = []
        
        for key, value_generator in keys_and_generators:
            task = self._preload_single(key, value_generator)
            tasks.append(task)
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def _preload_single(self, key: str, value_generator: Callable) -> None:
        """Preload single cache entry."""
        try:
            # Check if already cached
            existing = await self.get_async(key)
            if existing is not None:
                return
            
            # Generate value
            loop = asyncio.get_event_loop()
            value = await loop.run_in_executor(self.executor, value_generator)
            
            # Cache it
            await self.set_async(key, value)
            
        except Exception as e:
            self.logger.log_error("cache_preload", e, {"key": key})


def async_cached(ttl: int = 3600, persist: bool = True):
    """Decorator for async caching."""
    def decorator(func: Callable) -> Callable:
        cache_manager = AsyncCacheManager()
        
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # Generate cache key
            from .caching import cache_key
            key = f"{func.__module__}.{func.__name__}:{cache_key(*args, **kwargs)}"
            
            # Try to get from cache
            cached_result = await cache_manager.get_async(key)
            if cached_result is not None:
                return cached_result
            
            # Execute function
            if asyncio.iscoroutinefunction(func):
                result = await func(*args, **kwargs)
            else:
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, func, *args, **kwargs)
            
            # Cache result
            await cache_manager.set_async(key, result, ttl, persist)
            
            return result
        
        return wrapper
    return decorator


class RateLimiter:
    """Rate limiter for API calls and resource-intensive operations."""
    
    def __init__(self, max_calls: int = 10, time_window: float = 60.0):
        """Initialize rate limiter."""
        self.max_calls = max_calls
        self.time_window = time_window
        self.calls = []
        self.lock = threading.Lock()
        self.logger = SmellDiffusionLogger("rate_limiter")
    
    def is_allowed(self) -> bool:
        """Check if a call is allowed under rate limits."""
        with self.lock:
            now = time.time()
            
            # Remove old calls outside the time window
            self.calls = [call_time for call_time in self.calls 
                         if now - call_time < self.time_window]
            
            # Check if we can make another call
            if len(self.calls) < self.max_calls:
                self.calls.append(now)
                return True
            
            return False
    
    async def wait_if_needed(self) -> None:
        """Wait until a call is allowed."""
        while not self.is_allowed():
            await asyncio.sleep(0.1)
    
    def get_stats(self) -> Dict[str, Any]:
        """Get rate limiter statistics."""
        with self.lock:
            now = time.time()
            recent_calls = [call_time for call_time in self.calls 
                           if now - call_time < self.time_window]
            
            return {
                "current_calls": len(recent_calls),
                "max_calls": self.max_calls,
                "time_window": self.time_window,
                "utilization": len(recent_calls) / self.max_calls
            }


def rate_limited(max_calls: int = 10, time_window: float = 60.0):
    """Decorator to apply rate limiting to functions."""
    limiter = RateLimiter(max_calls, time_window)
    
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            await limiter.wait_if_needed()
            
            if asyncio.iscoroutinefunction(func):
                return await func(*args, **kwargs)
            else:
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, func, *args, **kwargs)
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            while not limiter.is_allowed():
                time.sleep(0.1)
            return func(*args, **kwargs)
        
        # Return appropriate wrapper based on function type
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    
    return decorator


class AsyncBatchProcessor:
    """Asynchronous batch processing with smart batching."""
    
    def __init__(self, batch_size: int = 5, max_concurrent_batches: int = 3):
        """Initialize async batch processor."""
        self.batch_size = batch_size
        self.max_concurrent_batches = max_concurrent_batches
        self.logger = SmellDiffusionLogger("async_batch")
        self.semaphore = asyncio.Semaphore(max_concurrent_batches)
    
    async def process_items(self, items: List[Any], processor_func: Callable, **kwargs) -> List[Any]:
        """Process items in async batches."""
        if not items:
            return []
        
        # Split into batches
        batches = [items[i:i + self.batch_size] 
                  for i in range(0, len(items), self.batch_size)]
        
        self.logger.logger.info(f"Processing {len(items)} items in {len(batches)} batches")
        
        # Process batches concurrently
        tasks = []
        for batch in batches:
            task = self._process_batch(batch, processor_func, **kwargs)
            tasks.append(task)
        
        batch_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Flatten results
        all_results = []
        for batch_result in batch_results:
            if isinstance(batch_result, Exception):
                self.logger.log_error("batch_processing", batch_result)
                continue
            
            if isinstance(batch_result, list):
                all_results.extend(batch_result)
            else:
                all_results.append(batch_result)
        
        return all_results
    
    async def _process_batch(self, batch: List[Any], processor_func: Callable, **kwargs) -> List[Any]:
        """Process a single batch with semaphore control."""
        async with self.semaphore:
            batch_results = []
            
            # Process batch items
            if asyncio.iscoroutinefunction(processor_func):
                # Async processor
                tasks = [processor_func(item, **kwargs) for item in batch]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result in results:
                    if isinstance(result, Exception):
                        self.logger.log_error("batch_item_processing", result)
                        batch_results.append(None)
                    else:
                        batch_results.append(result)
            else:
                # Sync processor - run in executor
                loop = asyncio.get_event_loop()
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    tasks = [
                        loop.run_in_executor(executor, processor_func, item, **kwargs)
                        for item in batch
                    ]
                    results = await asyncio.gather(*tasks, return_exceptions=True)
                    
                    for result in results:
                        if isinstance(result, Exception):
                            self.logger.log_error("batch_item_processing", result)
                            batch_results.append(None)
                        else:
                            batch_results.append(result)
            
            return batch_results


class CircuitBreaker:
    """Circuit breaker pattern for resilient async operations."""
    
    def __init__(self, failure_threshold: int = 5, recovery_timeout: float = 60.0):
        """Initialize circuit breaker."""
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
        self.lock = threading.Lock()
        self.logger = SmellDiffusionLogger("circuit_breaker")
    
    async def call(self, func: Callable, *args, **kwargs) -> Any:
        """Execute function through circuit breaker."""
        if not self._can_execute():
            raise Exception("Circuit breaker is OPEN")
        
        try:
            if asyncio.iscoroutinefunction(func):
                result = await func(*args, **kwargs)
            else:
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, func, *args, **kwargs)
            
            self._on_success()
            return result
            
        except Exception as e:
            self._on_failure()
            raise
    
    def _can_execute(self) -> bool:
        """Check if execution is allowed."""
        with self.lock:
            if self.state == "CLOSED":
                return True
            
            if self.state == "OPEN":
                if self.last_failure_time and time.time() - self.last_failure_time > self.recovery_timeout:
                    self.state = "HALF_OPEN"
                    return True
                return False
            
            if self.state == "HALF_OPEN":
                return True
            
            return False
    
    def _on_success(self) -> None:
        """Handle successful execution."""
        with self.lock:
            self.failure_count = 0
            self.state = "CLOSED"
    
    def _on_failure(self) -> None:
        """Handle failed execution."""
        with self.lock:
            self.failure_count += 1
            self.last_failure_time = time.time()
            
            if self.failure_count >= self.failure_threshold:
                self.state = "OPEN"
                self.logger.logger.warning(
                    f"Circuit breaker opened after {self.failure_count} failures"
                )
    
    def get_state(self) -> Dict[str, Any]:
        """Get circuit breaker state."""
        with self.lock:
            return {
                "state": self.state,
                "failure_count": self.failure_count,
                "failure_threshold": self.failure_threshold,
                "last_failure_time": self.last_failure_time
            }